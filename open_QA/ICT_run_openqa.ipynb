{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T08:47:27.438169Z",
     "start_time": "2021-03-16T08:47:26.057882Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "from io import open\n",
    "import math\n",
    "import collections\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "from operator import itemgetter\n",
    "\n",
    "from torch.utils.data import (DataLoader, TensorDataset, SequentialSampler)\n",
    "\n",
    "import transformers\n",
    "#from IR.mix_LM import Config as IRConfig, ICT\n",
    "from IR.mix_LM import Config as IRConfig, BertMultiTask_ICT as BertMultiTask\n",
    "\n",
    "from MRC.tokenization import FullTokenizer\n",
    "from MRC.modeling_mrc import Config as MRCConfig, MixTraining\n",
    "from MRC.korquad_utils import read_squad_examples, convert_examples_to_features\n",
    "\n",
    "RawResult = collections.namedtuple(\"RawResult\", [\"unique_id\", \"start_logits\", \"end_logits\"])\n",
    "\n",
    "def write_predictions(all_examples, all_features, all_results, n_best_size,\n",
    "                      max_answer_length):\n",
    "    \"\"\"Write final predictions to the json file and log-odds of null if needed.\"\"\"\n",
    "\n",
    "    example_index_to_features = collections.defaultdict(list)\n",
    "    for feature in all_features:\n",
    "        example_index_to_features[feature.example_index].append(feature)\n",
    "\n",
    "    unique_id_to_result = {}\n",
    "    for result in all_results:\n",
    "        unique_id_to_result[result.unique_id] = result\n",
    "    \n",
    "    #print(unique_id_to_result)\n",
    "    _PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "        \"PrelimPrediction\",\n",
    "        [\"feature_index\", \"start_index\", \"end_index\", \"start_logit\", \"end_logit\"])\n",
    "\n",
    "    all_predictions = collections.OrderedDict()\n",
    "    all_nbest_json = collections.OrderedDict()\n",
    "\n",
    "    for (example_index, example) in enumerate(all_examples):\n",
    "        features = example_index_to_features[example_index]\n",
    "\n",
    "        prelim_predictions = []\n",
    "        for (feature_index, feature) in enumerate(features):\n",
    "            result = unique_id_to_result[feature.unique_id]\n",
    "            start_indexes = _get_best_indexes(result.start_logits, n_best_size)\n",
    "            end_indexes = _get_best_indexes(result.end_logits, n_best_size)\n",
    "            # if we could have irrelevant answers, get the min score of irrelevant\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # We could hypothetically create invalid predictions, e.g., predict\n",
    "                    # that the start of the span is in the question. We throw out all\n",
    "                    # invalid predictions.\n",
    "                    if start_index >= len(feature.tokens):\n",
    "                        continue\n",
    "                    if end_index >= len(feature.tokens):\n",
    "                        continue\n",
    "                    if start_index not in feature.token_to_orig_map:\n",
    "                        continue\n",
    "                    if end_index not in feature.token_to_orig_map:\n",
    "                        continue\n",
    "                    if not feature.token_is_max_context.get(start_index, False):\n",
    "                        continue\n",
    "                    if end_index < start_index:\n",
    "                        continue\n",
    "                    length = end_index - start_index + 1\n",
    "                    if length > max_answer_length:\n",
    "                        continue\n",
    "                    prelim_predictions.append(\n",
    "                        _PrelimPrediction(\n",
    "                            feature_index=feature_index,\n",
    "                            start_index=start_index,\n",
    "                            end_index=end_index,\n",
    "                            start_logit=result.start_logits[start_index],\n",
    "                            end_logit=result.end_logits[end_index]))\n",
    "\n",
    "        prelim_predictions = sorted(\n",
    "            prelim_predictions,\n",
    "            key=lambda x: (x.start_logit + x.end_logit),\n",
    "            reverse=True)\n",
    "        \n",
    "        _NbestPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "            \"NbestPrediction\", [\"text\",\n",
    "                                \"start_logit\",\n",
    "                                \"end_logit\",\n",
    "                                \"start_index\"])\n",
    "\n",
    "        seen_predictions = {}\n",
    "        nbest = []\n",
    "        for pred in prelim_predictions:\n",
    "            if len(nbest) >= n_best_size:\n",
    "                break\n",
    "            feature = features[pred.feature_index]\n",
    "\n",
    "            orig_doc_start = feature.token_to_orig_map[pred.start_index]\n",
    "            orig_doc_end = feature.token_to_orig_map[pred.end_index]\n",
    "            try:\n",
    "                orig_text = feature.paragraph_text[feature.span_offset[orig_doc_start].start:(feature.span_offset[orig_doc_end].end)+1]\n",
    "            except IndexError:\n",
    "                print('index error')\n",
    "                continue\n",
    "                \n",
    "            if orig_text.endswith(\" \"):\n",
    "                orig_text = orig_text[:-1]\n",
    "            final_text = orig_text\n",
    "            if final_text in seen_predictions:\n",
    "                continue\n",
    "\n",
    "            seen_predictions[final_text] = True\n",
    "\n",
    "            nbest.append(\n",
    "                _NbestPrediction(\n",
    "                    text=final_text,\n",
    "                    start_logit=pred.start_logit,\n",
    "                    end_logit=pred.end_logit,\n",
    "                    start_index=feature.span_offset[orig_doc_start].start\n",
    "                ))\n",
    "\n",
    "        # In very rare edge cases we could have no valid predictions. So we\n",
    "        # just create a nonce prediction in this case to avoid failure.\n",
    "        if not nbest:\n",
    "            nbest.append(\n",
    "                _NbestPrediction(text=\"empty\", start_logit=0.0, end_logit=0.0, start_index=0))\n",
    "\n",
    "        assert len(nbest) >= 1\n",
    "\n",
    "        total_scores = []\n",
    "        best_non_null_entry = None\n",
    "        for entry in nbest:\n",
    "            total_scores.append(entry.start_logit + entry.end_logit)\n",
    "\n",
    "        probs = _compute_softmax(total_scores)\n",
    "\n",
    "        nbest_json = []\n",
    "        for (i, entry) in enumerate(nbest):\n",
    "            output = collections.OrderedDict()\n",
    "            output[\"text\"] = entry.text\n",
    "            output[\"probability\"] = probs[i]\n",
    "            output[\"start_logit\"] = entry.start_logit\n",
    "            output[\"end_logit\"] = entry.end_logit\n",
    "            output[\"start_index\"] = entry.start_index\n",
    "            nbest_json.append(output)\n",
    "\n",
    "        assert len(nbest_json) >= 1\n",
    "\n",
    "        answer = nbest_json[0][\"text\"]\n",
    "        all_predictions[example.qas_id] = answer\n",
    "        all_nbest_json[example.qas_id] = nbest_json\n",
    "\n",
    "    return all_predictions, all_nbest_json\n",
    "\n",
    "def _get_best_indexes(logits, n_best_size):\n",
    "    \"\"\"Get the n-best logits from a list.\"\"\"\n",
    "    index_and_score = sorted(enumerate(logits), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    best_indexes = []\n",
    "    for i in range(len(index_and_score)):\n",
    "        if i >= n_best_size:\n",
    "            break\n",
    "        best_indexes.append(index_and_score[i][0])\n",
    "    return best_indexes\n",
    "\n",
    "\n",
    "def _compute_softmax(scores):\n",
    "    \"\"\"Compute softmax probability over raw logits.\"\"\"\n",
    "    if not scores:\n",
    "        return []\n",
    "\n",
    "    max_score = None\n",
    "    for score in scores:\n",
    "        if max_score is None or score > max_score:\n",
    "            max_score = score\n",
    "\n",
    "    exp_scores = []\n",
    "    total_sum = 0.0\n",
    "    for score in scores:\n",
    "        x = math.exp(score - max_score)\n",
    "        exp_scores.append(x)\n",
    "        total_sum += x\n",
    "\n",
    "    probs = []\n",
    "    for score in exp_scores:\n",
    "        probs.append(score / total_sum)\n",
    "    return probs\n",
    "\n",
    "\n",
    "class OpenQA(object):\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.top_k = 10\n",
    "        self.mrc_batch_size = 64\n",
    "        self.seed = 42\n",
    "        self.query_len = 64\n",
    "        self.max_seq_length = 384\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        n_gpu = torch.cuda.device_count()\n",
    "        random.seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "        torch.manual_seed(self.seed)\n",
    "        if n_gpu > 0:\n",
    "            torch.cuda.manual_seed_all(self.seed)\n",
    "\n",
    "        self.doc_embedding_vectors = torch.load(\"pre_compute_vector/pre-compute.vec\").to(self.device)\n",
    "        with open(\"pre_compute_vector/paragraph.pkl\", \"rb\") as f:\n",
    "            self.paragraphs = pickle.load(f)\n",
    "\n",
    "        ir_config = IRConfig(\"IR/mix_LM_config.json\")\n",
    "        mrc_config = MRCConfig(\"MRC/config.json\")\n",
    "\n",
    "        self.tokenizer = transformers.ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "        self.mrc_tokenizer = FullTokenizer('MRC/vocab_wiki_news_32k_0227.txt', do_basic_tokenize=True)\n",
    "\n",
    "        ir_config.vocab_size = self.tokenizer.vocab_size\n",
    "        ir_config.max_seq_length = self.max_seq_length\n",
    "        \n",
    "        self.ir_model = BertMultiTask(ir_config, train_mode='ict')\n",
    "        self.ir_model.load_state_dict(torch.load('IR/model.bin'))  # IR 파인튜닝 모델 로딩\n",
    "        #self.ir_model.load_state_dict(torch.load('checkpoint/pe_5e-05_126_5_4.bin'))  # IR 파인튜닝 모델 로딩\n",
    "        \n",
    "\n",
    "        self.ir_model.to(self.device)\n",
    "        \n",
    "        self.mrc_model = MixTraining(mrc_config)\n",
    "        self.mrc_model.load_state_dict(torch.load('MRC/model.bin')) # MRC 파인튜닝 모델 로딩\n",
    "        self.mrc_model.to(self.device)\n",
    "\n",
    "        self.ir_model.eval()\n",
    "        self.mrc_model.eval()\n",
    "\n",
    "    def _ir_preproc(self, query):\n",
    "        query_tokens = self.tokenizer.tokenize(query)\n",
    "\n",
    "        max_tokens_for_que = self.query_len - 2\n",
    "\n",
    "        if len(query_tokens) > max_tokens_for_que:\n",
    "            query_tokens = query_tokens[0:max_tokens_for_que]\n",
    "        \n",
    "        q_tokens = []\n",
    "        q_tokens.append(self.tokenizer.cls_token)\n",
    "        for i in range(len(query_tokens)):\n",
    "            q_tokens.append(query_tokens[i])\n",
    "        q_tokens.append(self.tokenizer.sep_token)\n",
    "\n",
    "        q_input_ids = [self.tokenizer.convert_tokens_to_ids(t) for t in q_tokens]\n",
    "        q_input_mask = [1] * len(q_input_ids)\n",
    "\n",
    "        while len(q_input_ids) < self.query_len:\n",
    "            q_input_ids.append(0)\n",
    "            q_input_mask.append(0)\n",
    "\n",
    "        assert len(q_input_ids) == self.query_len\n",
    "        assert len(q_input_mask) == self.query_len\n",
    "        \n",
    "        q_input_ids = torch.tensor(q_input_ids, dtype=torch.long)\n",
    "        q_input_mask = torch.tensor(q_input_mask, dtype=torch.long)\n",
    "\n",
    "        return q_input_ids, q_input_mask\n",
    "\n",
    "    def __call__(self, query, doc_cat):\n",
    "        \n",
    "        # Information Retrieval Step\n",
    "        input_ids, input_mask = self._ir_preproc(query)\n",
    "\n",
    "        input_ids = input_ids.unsqueeze(0).to(self.device)\n",
    "        input_mask = input_mask.unsqueeze(0).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            q_encode = self.ir_model.bert(input_ids = input_ids, attention_mask = input_mask)\n",
    "            q_encode_pooled = self.ir_model.decoder(q_encode)\n",
    "\n",
    "        score = torch.matmul(q_encode_pooled, self.doc_embedding_vectors.t())\n",
    "        sorted_score = torch.argsort(score, dim=1, descending=True)\n",
    "\n",
    "        top_score = sorted_score[:, :self.top_k].squeeze(0)\n",
    "        \n",
    "        ir_paragraph = []\n",
    "        paragraph_offset = [0]\n",
    "        for i, s in enumerate(top_score):\n",
    "            ir_paragraph.append(self.paragraphs[s][1])\n",
    "            offset_end = len(ir_paragraph[i]) + 1\n",
    "            paragraph_offset.append(paragraph_offset[i] + offset_end)\n",
    "\n",
    "        if doc_cat:\n",
    "            input_paragraph = [' '.join(ir_paragraph)]\n",
    "        else:\n",
    "            input_paragraph = ir_paragraph\n",
    "            \n",
    "        #print(input_paragraph)\n",
    "\n",
    "        # Machine Reading Comprehension Step\n",
    "        eval_examples = read_squad_examples(query, input_paragraph, f_tokenizer=self.mrc_tokenizer)\n",
    "        eval_features = convert_examples_to_features(\n",
    "            examples=eval_examples,\n",
    "            tokenizer=self.mrc_tokenizer,\n",
    "            max_seq_length=512,\n",
    "            doc_stride=256,\n",
    "            max_query_length=64)\n",
    "\n",
    "        all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "        all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "        all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "        all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n",
    "        eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_example_index)\n",
    "        \n",
    "        eval_sampler = SequentialSampler(eval_data)\n",
    "        eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=self.mrc_batch_size)\n",
    "\n",
    "        all_results = []\n",
    "        for input_ids, input_mask, segment_ids, example_indices in eval_dataloader:\n",
    "            input_ids = input_ids.to(self.device)\n",
    "            input_mask = input_mask.to(self.device)\n",
    "            segment_ids = segment_ids.to(self.device)\n",
    "            with torch.no_grad():\n",
    "                batch_start_logits, batch_end_logits = self.mrc_model(input_ids, segment_ids, input_mask)\n",
    "            for i, example_index in enumerate(example_indices):\n",
    "                start_logits = batch_start_logits[i].detach().cpu().tolist()\n",
    "                end_logits = batch_end_logits[i].detach().cpu().tolist()\n",
    "                eval_feature = eval_features[example_index.item()]\n",
    "                unique_id = int(eval_feature.unique_id)\n",
    "                all_results.append(RawResult(unique_id=unique_id,\n",
    "                                             start_logits=start_logits,\n",
    "                                             end_logits=end_logits))\n",
    "\n",
    "        answer, nbest = write_predictions(eval_examples, eval_features, all_results,\n",
    "                                          n_best_size=5, max_answer_length=30)\n",
    "        if doc_cat:\n",
    "            nbest = nbest[0]\n",
    "        else:\n",
    "            nbest = [best[0] for best in nbest.values()]\n",
    "\n",
    "        output = []\n",
    "        for i, best in enumerate(nbest):\n",
    "            for j, start in enumerate(paragraph_offset):\n",
    "                if best[\"start_index\"] < start:\n",
    "                    break\n",
    "            output.append({\n",
    "                'answer': best[\"text\"],\n",
    "                'probability': best[\"probability\"],\n",
    "                'paragraph': ir_paragraph[j-1 if doc_cat else i]\n",
    "            })\n",
    "        output = sorted(output, key=itemgetter('probability'), reverse=True)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T08:47:50.832657Z",
     "start_time": "2021-03-16T08:47:27.441128Z"
    }
   },
   "outputs": [],
   "source": [
    "model = OpenQA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T08:48:30.095145Z",
     "start_time": "2021-03-16T08:48:29.293649Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 구글 본사의 위치는 어디인가요?\n",
      "정답 : 미국 캘리포니아 주 마운틴뷰(86.63%)\n",
      "구글 와이파이 ( Google WiFi ) 는 미국 캘리포니아 주 마운틴뷰에 배치된 자치 무선망 ( Municipal wireless network ) 이다 . 온전히 구글에 의해 투자되었으며 마운틴뷰 가로등에 주로 설치되어 있다 . 구글은 2010년까지 이 서비스를 무료로 유지하겠다고 언급했다 . 최초 서비스는 마운틴뷰 기지에서 2014년 5월 3일 구글에 의해 종료되었으며 새로운 공공 실외 와이파이를 제공하였다 .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Query: \")\n",
    "result = model(query, True)\n",
    "for i in result:\n",
    "    if i['probability'] > 0.1:\n",
    "        print('정답 :', i['answer']+'(%0.2f%%)' %(i['probability']*100))\n",
    "        print(i['paragraph'])\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T08:47:52.669488Z",
     "start_time": "2021-03-16T08:47:52.650031Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `대통령은` not found.\n",
      "Object `되나요` not found.\n",
      "Object `입니까` not found.\n",
      "Object `누구인가` not found.\n",
      "Object `사람은` not found.\n",
      "Object `창시자는` not found.\n",
      "Object `어디인가요` not found.\n"
     ]
    }
   ],
   "source": [
    "# 위키 2018년 덤프 중 약 20만개에서 리스트와 표를 제외한 텍스트로 구성함\n",
    "# 샘플 질문\n",
    "2018년 취임한 대한민국 대통령은?\n",
    "탄소와 산소가 반응하면 무엇이 되나요?\n",
    "지구의 반지름은 얼마 입니까?\n",
    "너 자신을 알라고 한 철학자는 누구인가?\n",
    "방사선을 최초로 발견한 사람은?\n",
    "애플의 창시자는?\n",
    "구글 본사의 위치는 어디인가요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
